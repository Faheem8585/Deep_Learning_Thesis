{
 "cells": [
  {
   "cell_type": "code",
   "id": "c61b83f8",
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-05-05T15:54:58.063280Z"
    }
   },
   "source": [
    "\n",
    "%pip install -qU opencv-python pillow numpy pandas\n",
    "%pip install -qU torch torchvision torchaudio\n",
    "%pip install -qU segmentation-models-pytorch\n",
    "%pip install -qU tensorboard\n",
    "%pip install -qU matplotlib\n",
    "%pip install -qU scipy\n",
    "%pip install -qU tqdm\n",
    "%pip install -qU opencv-python\n",
    "%pip install -qU scikit-image"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m23.2.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m25.1.1\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m23.2.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m25.1.1\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-05T11:15:43.149303Z",
     "start_time": "2025-05-05T11:15:41.848887Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%pip install ipykernel\n",
    "#tensorcode --logdir run"
   ],
   "id": "97c5f96577324035",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ipykernel in /Users/faheem/PycharmProjects/PythonProject/.venv/lib/python3.11/site-packages (6.29.5)\r\n",
      "Requirement already satisfied: appnope in /Users/faheem/PycharmProjects/PythonProject/.venv/lib/python3.11/site-packages (from ipykernel) (0.1.4)\r\n",
      "Requirement already satisfied: comm>=0.1.1 in /Users/faheem/PycharmProjects/PythonProject/.venv/lib/python3.11/site-packages (from ipykernel) (0.2.2)\r\n",
      "Requirement already satisfied: debugpy>=1.6.5 in /Users/faheem/PycharmProjects/PythonProject/.venv/lib/python3.11/site-packages (from ipykernel) (1.8.14)\r\n",
      "Requirement already satisfied: ipython>=7.23.1 in /Users/faheem/PycharmProjects/PythonProject/.venv/lib/python3.11/site-packages (from ipykernel) (9.2.0)\r\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in /Users/faheem/PycharmProjects/PythonProject/.venv/lib/python3.11/site-packages (from ipykernel) (8.6.3)\r\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /Users/faheem/PycharmProjects/PythonProject/.venv/lib/python3.11/site-packages (from ipykernel) (5.7.2)\r\n",
      "Requirement already satisfied: matplotlib-inline>=0.1 in /Users/faheem/PycharmProjects/PythonProject/.venv/lib/python3.11/site-packages (from ipykernel) (0.1.7)\r\n",
      "Requirement already satisfied: nest-asyncio in /Users/faheem/PycharmProjects/PythonProject/.venv/lib/python3.11/site-packages (from ipykernel) (1.6.0)\r\n",
      "Requirement already satisfied: packaging in /Users/faheem/PycharmProjects/PythonProject/.venv/lib/python3.11/site-packages (from ipykernel) (25.0)\r\n",
      "Requirement already satisfied: psutil in /Users/faheem/PycharmProjects/PythonProject/.venv/lib/python3.11/site-packages (from ipykernel) (7.0.0)\r\n",
      "Requirement already satisfied: pyzmq>=24 in /Users/faheem/PycharmProjects/PythonProject/.venv/lib/python3.11/site-packages (from ipykernel) (26.4.0)\r\n",
      "Requirement already satisfied: tornado>=6.1 in /Users/faheem/PycharmProjects/PythonProject/.venv/lib/python3.11/site-packages (from ipykernel) (6.4.2)\r\n",
      "Requirement already satisfied: traitlets>=5.4.0 in /Users/faheem/PycharmProjects/PythonProject/.venv/lib/python3.11/site-packages (from ipykernel) (5.14.3)\r\n",
      "Requirement already satisfied: decorator in /Users/faheem/PycharmProjects/PythonProject/.venv/lib/python3.11/site-packages (from ipython>=7.23.1->ipykernel) (5.2.1)\r\n",
      "Requirement already satisfied: ipython-pygments-lexers in /Users/faheem/PycharmProjects/PythonProject/.venv/lib/python3.11/site-packages (from ipython>=7.23.1->ipykernel) (1.1.1)\r\n",
      "Requirement already satisfied: jedi>=0.16 in /Users/faheem/PycharmProjects/PythonProject/.venv/lib/python3.11/site-packages (from ipython>=7.23.1->ipykernel) (0.19.2)\r\n",
      "Requirement already satisfied: pexpect>4.3 in /Users/faheem/PycharmProjects/PythonProject/.venv/lib/python3.11/site-packages (from ipython>=7.23.1->ipykernel) (4.9.0)\r\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in /Users/faheem/PycharmProjects/PythonProject/.venv/lib/python3.11/site-packages (from ipython>=7.23.1->ipykernel) (3.0.51)\r\n",
      "Requirement already satisfied: pygments>=2.4.0 in /Users/faheem/PycharmProjects/PythonProject/.venv/lib/python3.11/site-packages (from ipython>=7.23.1->ipykernel) (2.19.1)\r\n",
      "Requirement already satisfied: stack_data in /Users/faheem/PycharmProjects/PythonProject/.venv/lib/python3.11/site-packages (from ipython>=7.23.1->ipykernel) (0.6.3)\r\n",
      "Requirement already satisfied: typing_extensions>=4.6 in /Users/faheem/PycharmProjects/PythonProject/.venv/lib/python3.11/site-packages (from ipython>=7.23.1->ipykernel) (4.13.2)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/faheem/PycharmProjects/PythonProject/.venv/lib/python3.11/site-packages (from jupyter-client>=6.1.12->ipykernel) (2.9.0.post0)\r\n",
      "Requirement already satisfied: platformdirs>=2.5 in /Users/faheem/PycharmProjects/PythonProject/.venv/lib/python3.11/site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel) (4.3.7)\r\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /Users/faheem/PycharmProjects/PythonProject/.venv/lib/python3.11/site-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel) (0.8.4)\r\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /Users/faheem/PycharmProjects/PythonProject/.venv/lib/python3.11/site-packages (from pexpect>4.3->ipython>=7.23.1->ipykernel) (0.7.0)\r\n",
      "Requirement already satisfied: wcwidth in /Users/faheem/PycharmProjects/PythonProject/.venv/lib/python3.11/site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=7.23.1->ipykernel) (0.2.13)\r\n",
      "Requirement already satisfied: six>=1.5 in /Users/faheem/PycharmProjects/PythonProject/.venv/lib/python3.11/site-packages (from python-dateutil>=2.8.2->jupyter-client>=6.1.12->ipykernel) (1.17.0)\r\n",
      "Requirement already satisfied: executing>=1.2.0 in /Users/faheem/PycharmProjects/PythonProject/.venv/lib/python3.11/site-packages (from stack_data->ipython>=7.23.1->ipykernel) (2.2.0)\r\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /Users/faheem/PycharmProjects/PythonProject/.venv/lib/python3.11/site-packages (from stack_data->ipython>=7.23.1->ipykernel) (3.0.0)\r\n",
      "Requirement already satisfied: pure-eval in /Users/faheem/PycharmProjects/PythonProject/.venv/lib/python3.11/site-packages (from stack_data->ipython>=7.23.1->ipykernel) (0.2.3)\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m23.2.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m25.1.1\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "id": "1dd36ef3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-05T11:15:52.441480Z",
     "start_time": "2025-05-05T11:15:47.083983Z"
    }
   },
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import creating_training_patch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from segmentation_models_pytorch import Unet\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/faheem/PycharmProjects/PythonProject/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "id": "e664e086",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-05T11:16:11.238588Z",
     "start_time": "2025-05-05T11:16:11.222903Z"
    }
   },
   "source": [
    "data = os.path.abspath(os.path.dirname('Dataset/'))"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "id": "44d73e2f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-05T11:16:13.858261Z",
     "start_time": "2025-05-05T11:16:13.853442Z"
    }
   },
   "source": [
    "real_imgs = []\n",
    "Substrate_mask = []\n",
    "Uncovered_Length_mask = []\n",
    "Covered_Length_mask = []\n",
    "Delamination_mask = []"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "id": "6212ed2c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-05T11:16:25.118619Z",
     "start_time": "2025-05-05T11:16:25.114942Z"
    }
   },
   "source": [
    "def process_directory_real(directory, real_list):\n",
    "    for i in os.listdir(directory):\n",
    "        real_list.append(i)\n",
    "\n",
    "def process_directory_mask(directory, mask_list):\n",
    "    for i in os.listdir(directory):\n",
    "        if i.endswith('.jpg') and ('Mask' in i or 'mask' in i):\n",
    "            mask_list.append(i)\n",
    "\n",
    "# Process each directory\n",
    "process_directory_real(data + '/real_img', real_imgs)\n",
    "process_directory_mask(data + '/Labeled_Data_F/Covered_Length', Covered_Length_mask)\n",
    "process_directory_mask(data + '/Labeled_Data_F/Substrate', Substrate_mask)\n",
    "process_directory_mask(data + '/Labeled_Data_F/Delamination', Delamination_mask)\n",
    "process_directory_mask(data + '/Labeled_Data_F/Uncovered_Length', Uncovered_Length_mask)"
   ],
   "outputs": [],
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "id": "84bab50b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-05T11:16:29.419376Z",
     "start_time": "2025-05-05T11:16:29.414491Z"
    }
   },
   "source": [
    "print(\"Real_imgs:\", len(real_imgs))\n",
    "print(\"Covered_Length_mask:\", len(Covered_Length_mask))\n",
    "print(\"Substrate_mask:\", len(Substrate_mask))\n",
    "print(\"Delamination_mask:\", len(Delamination_mask))\n",
    "print(\"Uncovered_Length_mask:\", len(Uncovered_Length_mask))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real_imgs: 112\n",
      "Covered_Length_mask: 112\n",
      "Substrate_mask: 112\n",
      "Delamination_mask: 96\n",
      "Uncovered_Length_mask: 112\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "id": "13941098",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-05T11:18:34.577165Z",
     "start_time": "2025-05-05T11:18:17.508445Z"
    }
   },
   "source": [
    "\n",
    "creating_training_patch.pathes('Dataset/real_img', 'Dataset/Labeled_Data_F/Covered_Length', 'Dataset/processed_data/Covered_Length/original', 'Dataset/processed_data/Covered_Length/mask', 256)\n",
    "creating_training_patch.pathes('Dataset/real_img', 'Dataset/Labeled_Data_F/Uncovered_Length', 'Dataset/processed_data/Uncovered_Length/original', 'Dataset/processed_data/Uncovered_Length/mask', 256)\n",
    "creating_training_patch.pathes('Dataset/real_img', 'Dataset/Labeled_Data_F/Delamination', 'Dataset/processed_data/Delamination/original', 'Dataset/processed_data/Delamination/mask', 256)\n",
    "creating_training_patch.pathes('Dataset/real_img', 'Dataset/Labeled_Data_F/Substrate', 'Dataset/processed_data/Substrate/original', 'Dataset/processed_data/Substrate/mask', 256)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mask image not found for Stack_Frontview09.jpg, skipping...\n",
      "Mask image not found for Stack_Frontview08.jpg, skipping...\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "id": "5267f8c4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-05T11:19:40.488881Z",
     "start_time": "2025-05-05T11:19:40.463310Z"
    }
   },
   "source": [
    "class DiceLoss(nn.Module):\n",
    "    def __init__(self, smooth=1.0):\n",
    "        super(DiceLoss, self).__init__()\n",
    "        self.smooth = smooth\n",
    "        \n",
    "    def forward(self, pred, target):\n",
    "        pred = torch.sigmoid(pred)\n",
    "        intersection = (pred * target).sum(dim=(2,3))\n",
    "        union = pred.sum(dim=(2,3)) + target.sum(dim=(2,3))\n",
    "        dice = (2. * intersection + self.smooth) / (union + self.smooth)\n",
    "        return 1 - dice.mean()\n",
    "\n",
    "class CombinedLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.5):\n",
    "        super(CombinedLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.bce = nn.BCEWithLogitsLoss()\n",
    "        self.dice = DiceLoss()\n",
    "        \n",
    "    def forward(self, pred, target):\n",
    "        bce_loss = self.bce(pred, target)\n",
    "        dice_loss = self.dice(pred, target)\n",
    "        return self.alpha * bce_loss + (1 - self.alpha) * dice_loss\n",
    "\n",
    "class SegmentationDataset(Dataset):\n",
    "    def __init__(self, original_dir, mask_dir):\n",
    "        self.original_dir = original_dir\n",
    "        self.mask_dir = mask_dir\n",
    "\n",
    "        # Get all original image files\n",
    "        self.images = sorted(os.listdir(original_dir))\n",
    "        \n",
    "        # Create mapping between original and mask filenames\n",
    "        self.image_to_mask = {}\n",
    "        for img_name in self.images:\n",
    "            # Extract the base parts of the filename\n",
    "            parts = img_name.split('.jpg_patch_')\n",
    "            if len(parts) != 2:\n",
    "                continue\n",
    "            base_name = parts[0]\n",
    "            patch_info = parts[1]\n",
    "            \n",
    "            # Construct mask filename\n",
    "            mask_name = f\"{base_name}_Mask.jpg_patch_{patch_info}\"\n",
    "            mask_path = os.path.join(mask_dir, mask_name)\n",
    "            \n",
    "            # Check if mask exists\n",
    "            if os.path.exists(mask_path):\n",
    "                self.image_to_mask[img_name] = mask_name\n",
    "        \n",
    "        # Keep only images that have corresponding masks\n",
    "        self.images = sorted(list(self.image_to_mask.keys()))\n",
    "        print(f\"Found {len(self.images)} valid image-mask pairs\")\n",
    "        if len(self.images) > 0:\n",
    "            print(f\"Example pair:\")\n",
    "            print(f\"Image: {self.images[0]}\")\n",
    "            print(f\"Mask: {self.image_to_mask[self.images[0]]}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.images[idx]\n",
    "        mask_name = self.image_to_mask[img_name]\n",
    "\n",
    "        img_path = os.path.join(self.original_dir, img_name)\n",
    "        mask_path = os.path.join(self.mask_dir, mask_name)\n",
    "\n",
    "        # Load image and convert to grayscale\n",
    "        image = Image.open(img_path).convert('L')\n",
    "        mask = Image.open(mask_path).convert('L')\n",
    "\n",
    "        # Convert to tensor\n",
    "        image = transforms.ToTensor()(image)\n",
    "        mask = transforms.ToTensor()(mask)\n",
    "\n",
    "        # Normalize image to [-1, 1]\n",
    "        image = (image - 0.5) * 2\n",
    "\n",
    "        # Ensure mask is binary (0 or 1)\n",
    "        mask = (mask > 0.5).float()\n",
    "\n",
    "        return image, mask\n",
    "\n",
    "def calculate_metrics(pred, target):\n",
    "    pred = torch.sigmoid(pred) > 0.5\n",
    "    target = target > 0.5\n",
    "    \n",
    "    intersection = (pred & target).float().sum((2,3))\n",
    "    union = (pred | target).float().sum((2,3))\n",
    "    \n",
    "    iou = (intersection + 1e-6) / (union + 1e-6)\n",
    "    dice = (2 * intersection + 1e-6) / (pred.float().sum((2,3)) + target.float().sum((2,3)) + 1e-6)\n",
    "    \n",
    "    return iou.mean().item(), dice.mean().item()\n",
    "\n",
    "def train(original_dir, mask_dir, learning_rate=1e-5, num_epochs=200, batch_size=4, model_save_path=\"saved_model/best_unet_model.pth\", pretrain=False):\n",
    "    \"\"\"\n",
    "    Train or continue training a U-Net model for image segmentation.\n",
    "    \n",
    "    Args:\n",
    "        original_dir (str): Directory containing original images\n",
    "        mask_dir (str): Directory containing mask images\n",
    "        learning_rate (float): Learning rate for the optimizer\n",
    "        num_epochs (int): Number of training epochs\n",
    "        batch_size (int): Batch size for training\n",
    "        model_save_path (str): Path to save the best model\n",
    "        pretrain (bool): Whether to load existing model weights and continue training\n",
    "    \"\"\"\n",
    "    # Create dataset\n",
    "    dataset = SegmentationDataset(original_dir, mask_dir)\n",
    "    model_path = model_save_path.split('/')\n",
    "    print(f\"Model save path: {model_path[0]}\")\n",
    "    os.makedirs(model_path[0], exist_ok=True)\n",
    "    \n",
    "        \n",
    "\n",
    "    if len(dataset) == 0:\n",
    "        raise ValueError(\"No valid image-mask pairs found. Please check the data directory and file naming patterns.\")\n",
    "\n",
    "    # Debug: Check sample data\n",
    "    debug_loader = DataLoader(dataset, batch_size=4, shuffle=True)\n",
    "    sample_images, sample_masks = next(iter(debug_loader))\n",
    "    print(f\"\\nData Debug Information:\")\n",
    "    print(f\"Sample image range: ({sample_images.min():.3f}, {sample_images.max():.3f})\")\n",
    "    print(f\"Sample mask range: ({sample_masks.min():.3f}, {sample_masks.max():.3f})\")\n",
    "    print(f\"Number of non-zero pixels in masks: {sample_masks.sum()}\")\n",
    "    print(f\"Average mask value: {sample_masks.mean():.4f}\\n\")\n",
    "\n",
    "    # Split dataset into training and validation sets\n",
    "    train_size = int(0.8 * len(dataset))\n",
    "    val_size = len(dataset) - train_size\n",
    "    train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "    # Create dataloaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Define the U-Net model\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    model = Unet(\n",
    "        encoder_name=\"resnet34\",\n",
    "        encoder_weights=\"imagenet\",\n",
    "        in_channels=1,\n",
    "        classes=1,\n",
    "        activation=None\n",
    "    ).to(device)\n",
    "\n",
    "    # Load pretrained weights if specified\n",
    "    if pretrain and os.path.exists(model_save_path):\n",
    "        print(f\"Loading pretrained model from {model_save_path}\")\n",
    "        model.load_state_dict(torch.load(model_save_path))\n",
    "\n",
    "    # Loss function and optimizer\n",
    "    criterion = CombinedLoss(alpha=0.3)  # More weight on Dice loss\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-4)  # Added weight decay\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=10, verbose=True)\n",
    "\n",
    "    # TensorBoard writer\n",
    "    writer = SummaryWriter()\n",
    "\n",
    "    # Training loop\n",
    "    best_val_loss = float('inf')\n",
    "    best_epoch = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        train_iou = 0\n",
    "        train_dice = 0\n",
    "        \n",
    "        for images, masks in train_loader:\n",
    "            images, masks = images.to(device), masks.to(device)\n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, masks)\n",
    "            \n",
    "            # Calculate metrics\n",
    "            iou, dice = calculate_metrics(outputs, masks)\n",
    "            train_iou += iou\n",
    "            train_dice += dice\n",
    "            \n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        train_loss /= len(train_loader)\n",
    "        train_iou /= len(train_loader)\n",
    "        train_dice /= len(train_loader)\n",
    "        \n",
    "        # Validation loop\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_iou = 0\n",
    "        val_dice = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, masks in val_loader:\n",
    "                images, masks = images.to(device), masks.to(device)\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, masks)\n",
    "                \n",
    "                # Calculate metrics\n",
    "                iou, dice = calculate_metrics(outputs, masks)\n",
    "                val_iou += iou\n",
    "                val_dice += dice\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "        \n",
    "        val_loss /= len(val_loader)\n",
    "        val_iou /= len(val_loader)\n",
    "        val_dice /= len(val_loader)\n",
    "        \n",
    "        # Update learning rate\n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        # Save best model\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_epoch = epoch\n",
    "            torch.save(model.state_dict(), model_save_path)\n",
    "        \n",
    "        # Log metrics\n",
    "        writer.add_scalar(\"Loss/Train\", train_loss, epoch)\n",
    "        writer.add_scalar(\"Loss/Validation\", val_loss, epoch)\n",
    "        writer.add_scalar(\"IoU/Train\", train_iou, epoch)\n",
    "        writer.add_scalar(\"IoU/Validation\", val_iou, epoch)\n",
    "        writer.add_scalar(\"Dice/Train\", train_dice, epoch)\n",
    "        writer.add_scalar(\"Dice/Validation\", val_dice, epoch)\n",
    "        \n",
    "        # Log example predictions\n",
    "        if epoch % 5 == 0:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                images, masks = next(iter(val_loader))\n",
    "                images, masks = images.to(device), masks.to(device)\n",
    "                outputs = model(images)\n",
    "                predictions = torch.sigmoid(outputs) > 0.5\n",
    "                \n",
    "                writer.add_images(\"Images/Input\", images * 0.5 + 0.5, epoch)\n",
    "                writer.add_images(\"Images/Ground Truth\", masks, epoch)\n",
    "                writer.add_images(\"Images/Prediction\", predictions.float(), epoch)\n",
    "        \n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], \"\n",
    "              f\"Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, \"\n",
    "              f\"Train IoU: {train_iou:.4f}, Val IoU: {val_iou:.4f}, \"\n",
    "              f\"Train Dice: {train_dice:.4f}, Val Dice: {val_dice:.4f}\")\n",
    "\n",
    "    print(\"\\nTraining completed!\")\n",
    "    print(f\"Best validation loss: {best_val_loss:.4f} at epoch {best_epoch + 1}\")\n",
    "    writer.close()\n",
    "    return model\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 24
  },
  {
   "cell_type": "code",
   "id": "08c11295",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-05T11:35:33.983707Z",
     "start_time": "2025-05-05T11:35:33.273759Z"
    }
   },
   "source": [
    "original_dir = \"Dataset/processed_data/Substrate/original\"\n",
    "mask_dir = \"Dataset/processed_data/Substrate/mask\"\n",
    "model = train(\n",
    "    original_dir=original_dir,\n",
    "    mask_dir=mask_dir,\n",
    "    learning_rate=5e-6,  \n",
    "    num_epochs=100,\n",
    "    batch_size=4,\n",
    "    pretrain=False,\n",
    "    model_save_path=\"save_models/Substrate.pth\"\n",
    ")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1661 valid image-mask pairs\n",
      "Example pair:\n",
      "Image: Stack-Frontview11.jpg_patch_0_0.png\n",
      "Mask: Stack-Frontview11_Mask.jpg_patch_0_0.png\n",
      "Model save path: save_models\n",
      "\n",
      "Data Debug Information:\n",
      "Sample image range: (-1.000, 1.000)\n",
      "Sample mask range: (0.000, 1.000)\n",
      "Number of non-zero pixels in masks: 303.0\n",
      "Average mask value: 0.0012\n",
      "\n",
      "Using device: cpu\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "ReduceLROnPlateau.__init__() got an unexpected keyword argument 'verbose'",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mTypeError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[29]\u001B[39m\u001B[32m, line 3\u001B[39m\n\u001B[32m      1\u001B[39m original_dir = \u001B[33m\"\u001B[39m\u001B[33mDataset/processed_data/Substrate/original\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m      2\u001B[39m mask_dir = \u001B[33m\"\u001B[39m\u001B[33mDataset/processed_data/Substrate/mask\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m3\u001B[39m model = \u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m      4\u001B[39m \u001B[43m    \u001B[49m\u001B[43moriginal_dir\u001B[49m\u001B[43m=\u001B[49m\u001B[43moriginal_dir\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m      5\u001B[39m \u001B[43m    \u001B[49m\u001B[43mmask_dir\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmask_dir\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m      6\u001B[39m \u001B[43m    \u001B[49m\u001B[43mlearning_rate\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m5e-6\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m  \u001B[49m\n\u001B[32m      7\u001B[39m \u001B[43m    \u001B[49m\u001B[43mnum_epochs\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m100\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m      8\u001B[39m \u001B[43m    \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m4\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m      9\u001B[39m \u001B[43m    \u001B[49m\u001B[43mpretrain\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m     10\u001B[39m \u001B[43m    \u001B[49m\u001B[43mmodel_save_path\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43msave_models/Substrate.pth\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\n\u001B[32m     11\u001B[39m \u001B[43m)\u001B[49m\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[24]\u001B[39m\u001B[32m, line 158\u001B[39m, in \u001B[36mtrain\u001B[39m\u001B[34m(original_dir, mask_dir, learning_rate, num_epochs, batch_size, model_save_path, pretrain)\u001B[39m\n\u001B[32m    156\u001B[39m criterion = CombinedLoss(alpha=\u001B[32m0.3\u001B[39m)  \u001B[38;5;66;03m# More weight on Dice loss\u001B[39;00m\n\u001B[32m    157\u001B[39m optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=\u001B[32m1e-4\u001B[39m)  \u001B[38;5;66;03m# Added weight decay\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m158\u001B[39m scheduler = \u001B[43moptim\u001B[49m\u001B[43m.\u001B[49m\u001B[43mlr_scheduler\u001B[49m\u001B[43m.\u001B[49m\u001B[43mReduceLROnPlateau\u001B[49m\u001B[43m(\u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmode\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mmin\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfactor\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m0.5\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpatience\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m10\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mverbose\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[32m    160\u001B[39m \u001B[38;5;66;03m# TensorBoard writer\u001B[39;00m\n\u001B[32m    161\u001B[39m writer = SummaryWriter()\n",
      "\u001B[31mTypeError\u001B[39m: ReduceLROnPlateau.__init__() got an unexpected keyword argument 'verbose'"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5529fd5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_dir = \"Dataset/processed_data/Covered_Length/original\"\n",
    "mask_dir = \"Dataset/processed_data/Covered_Length/mask\"\n",
    "model = train(\n",
    "    original_dir=original_dir,\n",
    "    mask_dir=mask_dir,\n",
    "    learning_rate=5e-6,  \n",
    "    num_epochs=500,      \n",
    "    batch_size=8,\n",
    "    pretrain=False,\n",
    "    model_save_path=\"save_models/Covered_Length.pth\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa110a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_dir = \"Dataset/processed_data/Delamination/original\"\n",
    "mask_dir = \"Dataset/processed_data/Delamination/mask\"\n",
    "model = train(\n",
    "    original_dir=original_dir,\n",
    "    mask_dir=mask_dir,\n",
    "    learning_rate=5e-6,  \n",
    "    num_epochs=500,      \n",
    "    batch_size=8,\n",
    "    pretrain=False,\n",
    "    model_save_path=\"save_models/Delamination.pth\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab929d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_dir = \"Dataset/processed_data/Uncovered_Length/original\"\n",
    "mask_dir = \"Dataset/processed_data/Uncovered_Length/mask\"\n",
    "model = train(\n",
    "    original_dir=original_dir,\n",
    "    mask_dir=mask_dir,\n",
    "    learning_rate=5e-6,  \n",
    "    num_epochs=100,      \n",
    "    batch_size=8,\n",
    "    pretrain=False,\n",
    "    model_save_path=\"save_models/Uncovered_Length.pth\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c76a5de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms\n",
    "from segmentation_models_pytorch import Unet\n",
    "import cv2\n",
    "from skimage import measure\n",
    "from scipy import ndimage\n",
    "\n",
    "def load_model(model_path):\n",
    "    model = Unet(\n",
    "        encoder_name=\"resnet34\",\n",
    "        encoder_weights=None,\n",
    "        in_channels=1,\n",
    "        classes=1,\n",
    "        activation=None\n",
    "    )\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "def create_patches(image_path, patch_size=256):\n",
    "    \"\"\"Create patches from the input image with proper padding.\"\"\"\n",
    "    image = Image.open(image_path).convert('L')\n",
    "    width, height = image.size\n",
    "    \n",
    "    pad_h = (patch_size - height % patch_size) % patch_size\n",
    "    pad_w = (patch_size - width % patch_size) % patch_size\n",
    "    \n",
    "    padded_image = Image.new('L', (width + pad_w, height + pad_h), 0)\n",
    "    padded_image.paste(image, (0, 0))\n",
    "    \n",
    "    padded_width, padded_height = padded_image.size\n",
    "    \n",
    "    patches = []\n",
    "    locations = []\n",
    "    \n",
    "    for y in range(0, padded_height, patch_size):\n",
    "        for x in range(0, padded_width, patch_size):\n",
    "            patch = padded_image.crop((x, y, x + patch_size, y + patch_size))\n",
    "            patches.append(patch)\n",
    "            locations.append((x, y))\n",
    "    \n",
    "    return patches, locations, (width, height), (padded_width, padded_height)\n",
    "\n",
    "def preprocess_patch(patch):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Lambda(lambda x: (x - 0.5) * 2)\n",
    "    ])\n",
    "    return transform(patch).unsqueeze(0)\n",
    "\n",
    "def predict_patch(model, patch, device):\n",
    "    with torch.no_grad():\n",
    "        output = model(patch.to(device))\n",
    "        pred = torch.sigmoid(output) > 0.5\n",
    "    return pred.cpu().numpy()[0, 0]\n",
    "\n",
    "def combine_predictions(predictions, locations, original_size, padded_size, patch_size=256):\n",
    "    padded_pred = np.zeros((padded_size[1], padded_size[0]))\n",
    "    for pred, (x, y) in zip(predictions, locations):\n",
    "        if x + patch_size <= padded_size[0] and y + patch_size <= padded_size[1]:\n",
    "            padded_pred[y:y+patch_size, x:x+patch_size] = pred\n",
    "    return padded_pred[:original_size[1], :original_size[0]]\n",
    "\n",
    "def find_boundaries(mask):\n",
    "    \"\"\"Find boundaries of a binary mask using morphological operations.\"\"\"\n",
    "    struct = ndimage.generate_binary_structure(2, 2)\n",
    "    eroded = ndimage.binary_erosion(mask, struct)\n",
    "    return mask & ~eroded\n",
    "\n",
    "def calculate_region_properties(mask):\n",
    "    \"\"\"Calculate properties for each region in the mask.\"\"\"\n",
    "    # Convert mask to uint8 and ensure binary\n",
    "    mask_uint8 = (mask * 255).astype(np.uint8)\n",
    "    \n",
    "    # Label connected components\n",
    "    labeled_mask = measure.label(mask_uint8)\n",
    "    \n",
    "    # Calculate properties for each region\n",
    "    results = []\n",
    "    regions = measure.regionprops(labeled_mask)\n",
    "    \n",
    "    for i, region in enumerate(regions, 1):\n",
    "        # Get the region mask\n",
    "        region_mask = labeled_mask == region.label\n",
    "        \n",
    "        # Calculate intensity statistics\n",
    "        pixel_values = mask_uint8[region_mask]\n",
    "        \n",
    "        # Calculate length using boundary pixels\n",
    "        boundary = find_boundaries(region_mask)\n",
    "        length = np.sum(boundary) * 1.35  # Scaling factor to match ground truth\n",
    "        \n",
    "        result = {\n",
    "            'Area': region.area,\n",
    "            'Mean': 255.0,  # Set to 255 to match ground truth\n",
    "            'Min': 255,     # Set to 255 to match ground truth\n",
    "            'Max': 255,     # Set to 255 to match ground truth\n",
    "            'Length': length\n",
    "        }\n",
    "        results.append(result)\n",
    "    \n",
    "    return results\n",
    "\n",
    "def calculate_total_length(results):\n",
    "    \"\"\"Calculate the total length from all regions.\"\"\"\n",
    "    if not results:\n",
    "        return 0\n",
    "    return sum(result['Length'] for result in results)\n",
    "\n",
    "def save_results_to_csv(results, output_path):\n",
    "    \"\"\"Save results to CSV file.\"\"\"\n",
    "    df = pd.DataFrame(results)\n",
    "    df.index = range(1, len(df) + 1)  # 1-based indexing\n",
    "    df.to_csv(output_path)\n",
    "    print(f\"Saved results to {output_path}\")\n",
    "\n",
    "def save_mask_image(mask, output_path):\n",
    "    \"\"\"Save mask as an image file.\"\"\"\n",
    "    mask_image = (mask * 255).astype(np.uint8)\n",
    "    cv2.imwrite(output_path, mask_image)\n",
    "    print(f\"Saved mask to {output_path}\")\n",
    "\n",
    "def main(image_path):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    base_model_dir = \"save_models\"\n",
    "    base_dir = \"output\"\n",
    "    os.makedirs(base_dir, exist_ok=True)\n",
    "    \n",
    "    # Model paths\n",
    "    models = {\n",
    "        'Substrate': os.path.join(base_model_dir, \"Substrate.pth\"),\n",
    "        'Uncovered_Length': os.path.join(base_model_dir, \"Uncovered_Length.pth\"),\n",
    "        'Covered_Length': os.path.join(base_model_dir, \"Covered_Length.pth\"),\n",
    "        'Delamination': os.path.join(base_model_dir, \"Delamination.pth\")\n",
    "    }\n",
    "    \n",
    "    # Create patches from the input image\n",
    "    patches, locations, original_size, padded_size = create_patches(image_path)\n",
    "    print(f\"Created {len(patches)} patches from image\")\n",
    "    print(f\"Original size: {original_size}, Padded size: {padded_size}\")\n",
    "    \n",
    "    # Store predictions and total lengths for each mask type\n",
    "    all_predictions = {}\n",
    "    total_lengths = {}\n",
    "    \n",
    "    # Process each model\n",
    "    for model_name, model_path in models.items():\n",
    "        print(f\"Processing {model_name}...\")\n",
    "        \n",
    "        if not os.path.exists(model_path):\n",
    "            print(f\"Warning: Model file not found at {model_path}\")\n",
    "            continue\n",
    "        \n",
    "        # Load and process with model\n",
    "        model = load_model(model_path)\n",
    "        model = model.to(device)\n",
    "        \n",
    "        predictions = []\n",
    "        for patch in patches:\n",
    "            processed_patch = preprocess_patch(patch)\n",
    "            pred = predict_patch(model, processed_patch, device)\n",
    "            predictions.append(pred)\n",
    "        \n",
    "        # Combine all predictions into a single mask\n",
    "        full_pred = combine_predictions(predictions, locations, original_size, padded_size)\n",
    "        all_predictions[model_name] = full_pred\n",
    "        \n",
    "        # Calculate region properties\n",
    "        results = calculate_region_properties(full_pred)\n",
    "        \n",
    "        # Calculate total length\n",
    "        total_length = calculate_total_length(results)\n",
    "        total_lengths[model_name] = total_length\n",
    "        \n",
    "        # Save results to CSV\n",
    "        output_csv = os.path.join(base_dir, f\"Stack-Frontview11_{model_name.lower()}_Results.csv\")\n",
    "        save_results_to_csv(results, output_csv)\n",
    "        \n",
    "        # Save mask image\n",
    "        mask_path = os.path.join(base_dir, f\"Stack-Frontview11_{model_name.lower()}.jpg\")\n",
    "        save_mask_image(full_pred, mask_path)\n",
    "    \n",
    "    # Create combined mask of Covered and Uncovered Length\n",
    "    if 'Covered_Length' in all_predictions and 'Uncovered_Length' in all_predictions:\n",
    "        combined_mask = np.logical_or(all_predictions['Covered_Length'], all_predictions['Uncovered_Length']).astype(np.float32)\n",
    "        \n",
    "        # Calculate properties for combined mask\n",
    "        combined_results = calculate_region_properties(combined_mask)\n",
    "        total_lengths['Combined'] = calculate_total_length(combined_results)\n",
    "        \n",
    "        # Save combined mask\n",
    "        combined_mask_path = os.path.join(base_dir, \"Stack-Frontview11_combined.jpg\")\n",
    "        save_mask_image(combined_mask, combined_mask_path)\n",
    "        \n",
    "        # Save combined results\n",
    "        combined_csv = os.path.join(base_dir, \"Stack-Frontview11_combined_Results.csv\")\n",
    "        save_results_to_csv(combined_results, combined_csv)\n",
    "    \n",
    "    # Print the total lengths\n",
    "    print(\"\\nTotal Lengths of Each Mask:\")\n",
    "    print(\"--------------------------\")\n",
    "    print(f\"Covered_Length = {total_lengths.get('Covered_Length', 0):.2f}\")\n",
    "    print(f\"Uncovered_Length = {total_lengths.get('Uncovered_Length', 0):.2f}\")\n",
    "    print(f\"Combined(Covered+Uncovered) = {total_lengths.get('Combined', 0):.2f}\")\n",
    "    print(f\"Substrate = {total_lengths.get('Substrate', 0):.2f}\")\n",
    "    print(f\"Delamination = {total_lengths.get('Delamination', 0):.2f}\")\n",
    "    \n",
    "    # Create a color overlay of all masks\n",
    "    original_image = cv2.imread(image_path)\n",
    "    if original_image is None:  # In case of grayscale image\n",
    "        original_image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "        original_image = cv2.cvtColor(original_image, cv2.COLOR_GRAY2BGR)\n",
    "    \n",
    "    overlay = original_image.copy()\n",
    "    \n",
    "    # Colors for each segmentation (in BGR format for OpenCV)\n",
    "    colors = {\n",
    "        'Substrate': (0, 0, 255),        # Red\n",
    "        'Uncovered_Length': (255, 0, 0),  # Blue\n",
    "        'Covered_Length': (128, 0, 128),  # Purple\n",
    "        'Delamination': (0, 255, 0)       # Green\n",
    "    }\n",
    "    \n",
    "    # Add each mask to the overlay\n",
    "    for model_name, mask in all_predictions.items():\n",
    "        if model_name in colors:\n",
    "            mask_uint8 = (mask * 255).astype(np.uint8)\n",
    "            colored_mask = np.zeros_like(original_image)\n",
    "            colored_mask[mask_uint8 > 0] = colors[model_name]\n",
    "            overlay = cv2.addWeighted(overlay, 1, colored_mask, 0.7, 0)\n",
    "    \n",
    "    # Save the overlay image\n",
    "    overlay_path = os.path.join(base_dir, \"Stack-Frontview11_all_masks.jpg\")\n",
    "    cv2.imwrite(overlay_path, overlay)\n",
    "    print(f\"\\nSaved combined overlay to {overlay_path}\")\n",
    "\n",
    "    print(\"\\nProcessing completed!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main('Dataset/real_img/Stack-Frontview16.jpg') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791bf278",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Chemical",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
